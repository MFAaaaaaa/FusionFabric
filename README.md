# RISC-V-DPU
# RISC-V 高性能网络：引入 DPU 卸载的必要性研究
## Research on the Necessity of DPU Offloading for High-Performance RISC-V Networking

## 1. 项目简介 (Introduction)

本项目旨在研究并论证在 **RISC-V** 架构服务器中引入 **DPU (Data Processing Unit)** 以解决其在万兆网络环境下性能瓶颈的必要性。

随着 RISC-V 以其开放、可定制的特性在数据中心领域崭露头角，其网络 I/O 处理能力成为能否胜任高性能应用的关键。我们的基线性能测试表明，当前主流的 RISC-V 服务器在处理万兆网络流量时，面临着来自 **CPU 算力**和 **SoC I/O 通路**的双重瓶颈，导致资源浪费和性能不达标。

本项目提出将网络数据平面（Data Plane）从主 CPU 完全卸载到 DPU 的架构，以期从根本上解决这些问题，释放 RISC-V 核心的计算潜力。

## 2. 核心问题与痛点 (The Core Problem)

通过对 `UR1000` 和 `SG2044` 两款 RISC-V 服务器平台的详尽测试，我们识别出以下三大瓶颈：

### 瓶颈一：高昂的 CPU 软中断开销 (UR1000)
通用 CPU 在处理网络协议栈时效率低下，消耗了大量宝贵的计算资源。

> **证据**: `UR1000` 服务器在应对 X86 客户端时，虽然能达到 **9.41 Gbps** 的线速，但代价是高达 **17.82%** 的总 CPU 占用率。其中，**软中断（softirq）开销占比接近 30%**。作为对比，同等条件下的 X86 服务器仅用 **1.3%** 的 CPU 占用率就跑满了带宽。

这证明 RISC-V 通用核心在执行 Linux 内核网络协议栈时，效率远低于成熟的 X86 架构，网络 I/O 严重挤占了本应用于业务计算的资源。

### 瓶颈二：严重不均衡的单核中断处理 (UR1000)
在默认配置下，网络中断处理任务未能有效分配到多个核心，导致单核过载成为性能瓶颈。

> **证据**: 在单线程网络测试中，日志显示 `CPU 0` 核心独自处理了 **99.99%** 的网络接收软中断（`NET_RX`），而其他核心几乎完全空闲。这直接导致单线程吞吐量远无法达到线速。

这暴露了当前 RISC-V 平台的内核或驱动在多核负载均衡（如 RSS）方面存在配置或实现上的不足。

### 瓶颈三：非 CPU 的 I/O 吞吐上限 (SG2044)
`SG2044` 平台暴露出一个独立于 CPU 性能的“硬顶”瓶颈。

> **证据**: 无论客户端如何，也无论测试线程数多少，`SG2044` 的网络吞吐量始终被限制在 **~6.2 Gbps** 左右。而此时，其 **CPU 总占用率仅为 3%** 左右，资源极其充裕。

这强烈表明瓶颈不在 CPU 算力，而在于 SoC（片上系统）内部的数据通路，例如内存控制器带宽、PCIe 总线效率或网卡驱动/固件的内部限制，导致数据无法以线速在网卡和内存之间搬运。

## 3. 解决方案：网络平面卸载至 DPU (Our Solution: Offloading to DPU)

针对上述瓶颈，我们提出的解决方案是将整个网络数据平面从 RISC-V 主 CPU 中剥离，完全卸载到专用的 DPU 上。

利用 DPU 内置的硬件加速引擎和专用核心来处理网络协议、中断、数据包转发等任务，从而绕过主 CPU 的处理瓶颈和 SoC 的 I/O 瓶颈。

## 4. 初步验证：使用 XDP 模拟 DPU 卸载 (Preliminary Validation: Simulating DPU Offload with XDP)

为了在软件层面验证“卸载”这一核心思想的有效性，我们使用了 **XDP (eXpress Data Path)** 技术。XDP 允许我们在网络驱动层面对数据包进行极速处理，从而**模拟 DPU 在数据链路层绕过主机内核协议栈**的行为。

| XDP 程序 (Program) | 发包速率 (PPS) | 总CPU使用率 | 核心0总使用率 | 核心思想 |
| :----------------- | :------------- | :---------- | :------------ | :--------------------------------------------------- |
| **`xdp_pass`**     | 10M            | 8.36%       | **~66.9%**      | **基线**: 将包交给内核，CPU开销巨大，复现了瓶颈问题。  |
| **`xdp_drop`**     | 10M            | 0.62%       | **3.36%**       | **模拟过滤**: 在驱动层丢弃包，CPU开销极低。              |
|                    | 50M            | 0.65%       | **3.92%**       | 即使PPS增加5倍，CPU开销几乎不变，证明卸载效率。      |
| **`xdp_router`**   | 10M            | 4.75%       | **~38.0%**      | **模拟转发**: 在驱动层转发包，CPU开销远低于基线。        |

**分析结论:**
*   `xdp_pass` 的结果表明，仅仅将数据包上送给内核协议栈，就会立刻导致 CPU 核心（Core 0）不堪重负，这与我们之前发现的软中断瓶颈完全一致。
*   相比之下，`xdp_drop` 和 `xdp_router` 通过在驱动层直接处理数据包，**极大地降低了 CPU 的负载**。特别是在 `xdp_drop` 测试中，CPU 使用率几乎可以忽略不计。

这些 XDP 测试提供了强有力的软件证据，证明**将数据平面处理任务从内核中卸载是解决 RISC-V 网络性能瓶颈的正确方向**。它也预示了，使用专门的硬件（DPU）来执行这些卸载任务，将会获得更彻底、更高效的性能提升。

## 5. 预期效果 (Expected Outcomes)

通过引入 DPU，我们预期将实现：

*   **解放 CPU (CPU Freedom)**: RISC-V 主 CPU 处理网络相关的系统调用和软中断开销**降至接近于 0**，使其能够 100% 专注于应用负载。
*   **性能达标 (Line-Rate Performance)**: 无论是大包吞吐量还是小包处理能力（PPS），系统均能轻松达到**万兆线速**，彻底规避现有瓶颈。
*   **单线程线速 (Single-Thread Line-Rate)**: 由于数据路径被 DPU 完全接管，**单线程应用也能无差别地享受到完整的万兆带宽**，解决了因中断不均衡导致的单线程性能受限问题。

## 6. 关键实验数据摘要 (Key Experimental Data)

(此部分可保留作为附录或详细数据参考)

### UR1000 平台：CPU 成为瓶颈
| 客户端 (Client) | 并行线程 | 服务端带宽 (Gbps) | 服务端CPU占用率 | 内核态(sys)占比 | 软中断(soft)占比 |
| --------------- | -------- | ----------------- | --------------- | --------------- | ---------------- |
| **X86_24**      | 8        | **9.42**          | **25.35%**      | 49.3%           | **49.9%**        |

### SG2044 平台：SoC I/O 成为瓶颈
| 客户端 (Client) | 并行线程 | 服务端带宽 (Gbps) | 服务端CPU占用率 | 内核态(sys)占比 | 软中断(soft)占比 |
| --------------- | -------- | ----------------- | --------------- | --------------- | ---------------- |
| **UR1000**      | 8        | **6.09**          | **3.16%**       | 48.10%          | 48.70%           |

## 7. 测试环境 (Testbed)

*   **RISC-V 服务器**:
    *   `UR1000`
    *   `SG2042` / `SG2044`
*   **X86 服务器**:
    *   `X86_24`
    *   `X86_22`
*   **网络环境**:
    *   千兆及万兆以太网
*   **测试工具**:
    *   `iperf3`, `sockperf`, `ping`
    *   XDP/eBPF tools
    *   Linux `proc` 文件系统 (`/proc/softirqs`, `/proc/stat`), `mpstat`
