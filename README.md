# RISC-V-DPU
# RISC-V 高性能网络：引入 DPU 卸载的必要性研究
## Research on the Necessity of DPU Offloading for High-Performance RISC-V Networking

## 1. 项目简介 (Introduction)

本项目旨在研究并论证在 **RISC-V** 架构服务器中引入 **DPU (Data Processing Unit)** 以解决其在万兆网络环境下性能瓶颈的必要性。

随着 RISC-V 以其开放、可定制的特性在数据中心领域崭露头角，其网络 I/O 处理能力成为能否胜任高性能应用的关键。我们的基线性能测试表明，当前主流的 RISC-V 服务器在处理万兆网络流量时，面临着来自 **CPU 算力**和 **SoC I/O 通路**的双重瓶颈，导致资源浪费和性能不达标。

本项目提出将网络数据平面（Data Plane）从主 CPU 完全卸载到 DPU 的架构，以期从根本上解决这些问题，释放 RISC-V 核心的计算潜力。

## 2. 核心问题与痛点 (The Core Problem)

通过对 `UR1000` 和 `SG2044` 两款 RISC-V 服务器平台的详尽测试，我们识别出以下三大瓶颈：

### 瓶颈一：高昂的 CPU 软中断开销 (UR1000)
通用 CPU 在处理网络协议栈时效率低下，消耗了大量宝贵的计算资源。

> **证据**: `UR1000` 服务器在应对 X86 客户端时，虽然能达到 **9.41 Gbps** 的线速，但代价是高达 **17.82%** 的总 CPU 占用率。其中，**软中断（softirq）开销占比接近 30%**。作为对比，同等条件下的 X86 服务器仅用 **1.3%** 的 CPU 占用率就跑满了带宽。

这证明 RISC-V 通用核心在执行 Linux 内核网络协议栈时，效率远低于成熟的 X86 架构，网络 I/O 严重挤占了本应用于业务计算的资源。

### 瓶颈二：严重不均衡的单核中断处理 (UR1000)
在默认配置下，网络中断处理任务未能有效分配到多个核心，导致单核过载成为性能瓶颈。

> **证据**: 在单线程网络测试中，日志显示 `CPU 0` 核心独自处理了 **99.99%** 的网络接收软中断（`NET_RX`），而其他核心几乎完全空闲。这直接导致单线程吞吐量远无法达到线速。

这暴露了当前 RISC-V 平台的内核或驱动在多核负载均衡（如 RSS）方面存在配置或实现上的不足。

### 瓶颈三：非 CPU 的 I/O 吞吐上限 (SG2044)
`SG2044` 平台暴露出一个独立于 CPU 性能的“硬顶”瓶颈。

> **证据**: 无论客户端如何，也无论测试线程数多少，`SG2044` 的网络吞吐量始终被限制在 **~6.2 Gbps** 左右。而此时，其 **CPU 总占用率仅为 3%** 左右，资源极其充裕。

这强烈表明瓶颈不在 CPU 算力，而在于 SoC（片上系统）内部的数据通路，例如内存控制器带宽、PCIe 总线效率或网卡驱动/固件的内部限制，导致数据无法以线速在网卡和内存之间搬运。

## 3. 解决方案：网络平面卸载至 DPU (Our Solution: Offloading to DPU)

针对上述瓶颈，我们提出的解决方案是将整个网络数据平面从 RISC-V 主 CPU 中剥离，完全卸载到专用的 DPU 上。

利用 DPU 内置的硬件加速引擎和专用核心来处理网络协议、中断、数据包转发等任务，从而绕过主 CPU 的处理瓶颈和 SoC 的 I/O 瓶颈。

## 4. 预期效果 (Expected Outcomes)

通过引入 DPU，我们预期将实现：

*   **解放 CPU (CPU Freedom)**: RISC-V 主 CPU 处理网络相关的系统调用和软中断开销**降至接近于 0**，使其能够 100% 专注于应用负载。
*   **性能达标 (Line-Rate Performance)**: 无论是大包吞吐量还是小包处理能力（PPS），系统均能轻松达到**万兆线速**，彻底规避现有瓶颈。
*   **单线程线速 (Single-Thread Line-Rate)**: 由于数据路径被 DPU 完全接管，**单线程应用也能无差别地享受到完整的万兆带宽**，解决了因中断不均衡导致的单线程性能受限问题。

## 5. 关键实验数据摘要 (Key Experimental Data)

### UR1000 平台：CPU 成为瓶颈

| 客户端 (Client) | 并行线程 | 服务端带宽 (Gbps) | 服务端CPU占用率 | 内核态(sys)占比 | 软中断(soft)占比 |
| --------------- | -------- | ----------------- | --------------- | --------------- | ---------------- |
| **X86_24**      | 4        | **9.41**          | **17.82%**      | 67.9%           | **29.0%**        |
| **X86_24**      | 8        | **9.42**          | **25.35%**      | 49.3%           | **49.9%**        |

*结论：CPU 占用率随着流量增加而急剧攀升，软中断是主要的开销来源。*

### SG2044 平台：SoC I/O 成为瓶颈

| 客户端 (Client) | 并行线程 | 服务端带宽 (Gbps) | 服务端CPU占用率 | 内核态(sys)占比 | 软中断(soft)占比 |
| --------------- | -------- | ----------------- | --------------- | --------------- | ---------------- |
| **UR1000**      | 1        | **6.19**          | **3.40%**       | 47.60%          | 45.60%           |
| **UR1000**      | 8        | **6.09**          | **3.16%**       | 48.10%          | 48.70%           |
| **X86_24**      | 8        | **6.27**          | **3.12%**       | 48.70%          | 49.40%           |

*结论：在 CPU 资源极其充裕的情况下，带宽被锁定在 6.2Gbps 左右，暴露了非 CPU 的硬件瓶颈。*

### 中断不均衡问题

**RPS (Receive Packet Steering) 关闭状态**
| CPU 核心 | 处理中断数 | 处理占比 |
| -------- | ---------- | -------- |
| **CPU 0**  | **541,691**  | **99.99%** |
| CPU 1-7  | < 10       | < 0.01%  |

*结论：单个核心承载了几乎所有的网络中断处理，造成了严重的性能瓶颈。*

## 6. 测试环境 (Testbed)

*   **RISC-V 服务器**:
    *   `UR1000`
    *   `SG2042` / `SG2044`
*   **X86 服务器**:
    *   `X86_24`
    *   `X86_22`
*   **网络环境**:
    *   千兆及万兆以太网
*   **测试工具**:
    *   `iperf3`
    *   `sockperf`
    *   `ping`
    *   Linux `proc` 文件系统 (e.g., `/proc/softirqs`, `/proc/stat`)
    *   `mpstat`
